{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Final-Year-Project\\\\Credit-Card-Fraud-Detection-Using-GNN'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"../\")\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entity\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    root_dir: Path\n",
    "    data_path: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Credit_Card_Fraud_Detection.constants import *\n",
    "from Credit_Card_Fraud_Detection.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "\n",
    "class ConfigurationManager:\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath = CONFIG_FILE_PATH,\n",
    "        params_filepath = PARAMS_FILE_PATH,\n",
    "        schema_filepath = SCHEMA_FILE_PATH):\n",
    "\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "\n",
    "    \n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        config = self.config.data_transformation\n",
    "\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        data_transformation_config = DataTransformationConfig(\n",
    "            root_dir=config.root_dir,\n",
    "            data_path=config.data_path,\n",
    "        )\n",
    "\n",
    "        return data_transformation_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from Credit_Card_Fraud_Detection import logger\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "# import os\n",
    "# import logging\n",
    "# import numpy as np\n",
    "# from geopy.distance import geodesic\n",
    "\n",
    "# # Configure logging to see what the code is doing\n",
    "# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "# logger = logging.getLogger(__name__)\n",
    "\n",
    "# class DataTransformation:\n",
    "#     def __init__(self, config):\n",
    "#         self.config = config\n",
    "#         self.label_encoders = {}\n",
    "#         self.scaler = StandardScaler()\n",
    "#         self.customer_mapping = {}\n",
    "#         self.merchant_mapping = {}\n",
    "\n",
    "#     def load_data(self):\n",
    "#         try:\n",
    "#             df = pd.read_csv(self.config.data_path)\n",
    "#             logger.info(\"Data loaded successfully.\") # Tells us the data was loaded\n",
    "#             return df\n",
    "#         except FileNotFoundError:\n",
    "#             logger.error(f\"File not found: {self.config.data_path}\") # Tells us if the file wasn't found\n",
    "#             return None\n",
    "\n",
    "#     def drop_unwanted_columns(self, df):\n",
    "#         # We don't need some columns, so we remove them\n",
    "#         columns_to_drop = [\"merch_zipcode\", \"Unnamed: 0\", \"trans_num\", \"unix_time\", \"zip\", \"street\", \"city\", \"job\", \"first\", \"last\"]\n",
    "#         df = df.drop(columns=columns_to_drop, errors=\"ignore\")\n",
    "#         logger.info(f\"Unwanted columns {columns_to_drop} dropped.\") # Tells us what columns were removed\n",
    "#         return df\n",
    "\n",
    "#     def handle_missing_categorical(self, df):\n",
    "#         # If some categories or states are missing, we fill them with \"unknown\"\n",
    "#         df[\"category\"] = df[\"category\"].fillna(\"unknown\")\n",
    "#         df[\"state\"] = df[\"state\"].fillna(\"unknown\")\n",
    "#         logger.info(\"Missing categorical values handled.\") # Tells us missing values were filled\n",
    "#         return df\n",
    "\n",
    "#     def handle_numerical_features(self, df):\n",
    "#         # We make sure numbers are numbers, and fill missing numbers with 0\n",
    "#         num_features = [\"amt\", \"city_pop\", \"lat\", \"long\", \"merch_lat\", \"merch_long\"]\n",
    "#         existing_features = [col for col in num_features if col in df.columns]\n",
    "#         df[existing_features] = df[existing_features].apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "#         logger.info(\"Numerical features converted and missing values handled.\") # Tells us numbers were fixed\n",
    "#         return df\n",
    "\n",
    "#     def normalize_numerical_features(self, df):\n",
    "#         # We make the numbers have similar scales\n",
    "#         num_features = [\"amt\", \"city_pop\", \"lat\", \"long\", \"merch_lat\", \"merch_long\"]\n",
    "#         existing_features = [col for col in num_features if col in df.columns]\n",
    "#         df[existing_features] = self.scaler.fit_transform(df[existing_features])\n",
    "#         for col in [\"lat\", \"long\", \"merch_lat\", \"merch_long\"]:\n",
    "#             if col in df.columns:\n",
    "#                 df[col] = df[col].round(3)\n",
    "#         logger.info(\"Numerical features normalized.\") # Tells us the numbers were scaled\n",
    "#         return df\n",
    "\n",
    "#     def create_customer_and_merchant_ids(self, df):\n",
    "#         # We give each customer and merchant a unique number\n",
    "#         unique_customers = df['cc_num'].unique()\n",
    "#         self.customer_mapping = {customer: idx for idx, customer in enumerate(unique_customers)}\n",
    "#         df['customer_id'] = df['cc_num'].map(self.customer_mapping).astype(int)\n",
    "#         unique_merchants = df['merchant'].unique()\n",
    "#         self.merchant_mapping = {merchant: idx + len(unique_customers) for idx, merchant in enumerate(unique_merchants)}\n",
    "#         df['merchant_id'] = df['merchant'].map(self.merchant_mapping).astype(int)\n",
    "#         df.drop(columns=['cc_num', 'merchant'], inplace=True)\n",
    "#         logger.info(\"Customer and merchant IDs created.\") # Tells us IDs were created\n",
    "#         return df\n",
    "\n",
    "#     def extract_datetime_components(self, df):\n",
    "#         # We break down the date and time into year, month, day, etc.\n",
    "#         if 'trans_date_trans_time' in df.columns:\n",
    "#             df['trans_date_trans_time'] = pd.to_datetime(df['trans_date_trans_time'])\n",
    "#             df['trans_year'] = df['trans_date_trans_time'].dt.year\n",
    "#             df['trans_month'] = df['trans_date_trans_time'].dt.month\n",
    "#             df['trans_day'] = df['trans_date_trans_time'].dt.day\n",
    "#             df['trans_hour'] = df['trans_date_trans_time'].dt.hour\n",
    "#             df['trans_minute'] = df['trans_date_trans_time'].dt.minute\n",
    "#             df['trans_second'] = df['trans_date_trans_time'].dt.second\n",
    "#             df['trans_weekday'] = df['trans_date_trans_time'].dt.day_name()\n",
    "#             def get_season(month):\n",
    "#                 if month in [3, 4, 5]: return 'Spring'\n",
    "#                 elif month in [6, 7, 8]: return 'Summer'\n",
    "#                 elif month in [9, 10, 11]: return 'Autumn'\n",
    "#                 else: return 'Winter'\n",
    "#             df['trans_season'] = df['trans_month'].apply(get_season)\n",
    "#             df.drop('trans_date_trans_time', axis=1, inplace=True)\n",
    "#             logger.info(\"Datetime components extracted and trans_date_trans_time dropped.\") # Tells us time was broken down\n",
    "#         else:\n",
    "#             logger.error(\"trans_date_trans_time column not found.\") # Tells us if time column is missing\n",
    "#         return df\n",
    "\n",
    "#     def calculate_age(self, df):\n",
    "#         # We calculate the age of the customer\n",
    "#         if 'dob' in df.columns and 'trans_year' in df.columns:\n",
    "#             df['dob'] = pd.to_datetime(df['dob'])\n",
    "#             df['age'] = (df['trans_year'] - df['dob'].dt.year) - (\n",
    "#                 (df['trans_month'] < df['dob'].dt.month) |\n",
    "#                 ((df['trans_month'] == df['dob'].dt.month) & (df['trans_day'] < df['dob'].dt.day))\n",
    "#             )\n",
    "#             df.drop('dob', axis=1, inplace=True)\n",
    "#             logger.info(\"Age calculated from dob and trans_year.\") # Tells us age was calculated\n",
    "#         else:\n",
    "#             logger.error(\"dob or trans_year column not found.\") # Tells us if age columns are missing\n",
    "#         return df\n",
    "\n",
    "#     def encode_data(self, df):\n",
    "#         # We turn categories and states into numbers\n",
    "#         if \"gender\" in df.columns:\n",
    "#             df[\"gender\"] = df[\"gender\"].map({\"M\": 1, \"F\": 0})\n",
    "#             logger.info(\"Gender column encoded (M -> 1, F -> 0).\") # Tells us gender was encoded\n",
    "#         for col in [\"category\", \"state\"]:\n",
    "#             if col not in self.label_encoders:\n",
    "#                 self.label_encoders[col] = LabelEncoder()\n",
    "#                 df[col] = self.label_encoders[col].fit_transform(df[col])\n",
    "#             else:\n",
    "#                 df[col] = self.label_encoders[col].transform(df[col])\n",
    "#         logger.info(\"Categorical columns category and state encoded.\") # Tells us categories were encoded\n",
    "#         if \"trans_weekday\" in df.columns:\n",
    "#             weekday_mapping = {\n",
    "#                 \"Monday\": 0, \"Tuesday\": 1, \"Wednesday\": 2, \"Thursday\": 3,\n",
    "#                 \"Friday\": 4, \"Saturday\": 5, \"Sunday\": 6\n",
    "#             }\n",
    "#             df[\"trans_weekday\"] = df[\"trans_weekday\"].map(weekday_mapping)\n",
    "#             logger.info(\"trans_weekday column encoded.\") # Tells us day of week was encoded\n",
    "#         if \"trans_season\" in df.columns:\n",
    "#             season_mapping = {\"Spring\": 0, \"Summer\": 1, \"Autumn\": 2, \"Winter\": 3}\n",
    "#             df[\"trans_season\"] = df[\"trans_season\"].map(season_mapping)\n",
    "#             logger.info(\"trans_season column encoded.\") # Tells us season was encoded\n",
    "#         return df\n",
    "\n",
    "#     def feature_engineering(self, df):\n",
    "#         # We create new features to help the model learn better\n",
    "#         # Transaction Frequency and Recency (How often and how recently)\n",
    "#         df['customer_trans_freq'] = df.groupby('customer_id')['transaction_unique'].transform('count') # How many transactions per customer\n",
    "#         df['merchant_trans_freq'] = df.groupby('merchant_id')['transaction_unique'].transform('count') # How many transactions per merchant\n",
    "#         df['customer_last_trans'] = df.groupby('customer_id')['trans_year'].transform('max') # Last transaction year per customer\n",
    "#         df['customer_trans_recency'] = df['trans_year'] - df['customer_last_trans'] # How recent the customer's last transaction was\n",
    "\n",
    "#         # Time Based Features (Time of day, weekend)\n",
    "#         def time_of_day(hour):\n",
    "#             if 6 <= hour < 12: return 'Morning'\n",
    "#             elif 12 <= hour < 18: return 'Afternoon'\n",
    "#             elif 18 <= hour < 24: return 'Evening'\n",
    "#             else: return 'Night'\n",
    "#         df['time_of_day'] = df['trans_hour'].apply(time_of_day)\n",
    "#         df = pd.get_dummies(df, columns=['time_of_day'], prefix='time_of_day')  # one hot encoding\n",
    "#         for col in ['time_of_day_Afternoon', 'time_of_day_Evening', 'time_of_day_Morning', 'time_of_day_Night']:\n",
    "#             if col in df.columns:\n",
    "#                 df[col] = df[col].astype(int)\n",
    "\n",
    "#         df['is_weekend'] = df['trans_weekday'].apply(lambda x: 1 if x in ['Saturday', 'Sunday'] else 0)\n",
    "\n",
    "#         # Location Based Features (Distance, state frequency)\n",
    "#         def calculate_distance(row):\n",
    "#             try:\n",
    "#                 customer_coords = (row['lat'], row['long'])\n",
    "#                 merchant_coords = (row['merch_lat'], row['merch_long'])\n",
    "#                 return geodesic(customer_coords, merchant_coords).km\n",
    "#             except ValueError:\n",
    "#                 return np.nan  # Handle cases with missing lat/long\n",
    "\n",
    "#         df['distance_customer_merchant'] = df.apply(calculate_distance, axis=1) # Distance between customer and merchant\n",
    "#         df['state_freq'] = df.groupby('state')['transaction_unique'].transform('count') # How often transactions happen in each state\n",
    "\n",
    "#         # Transaction Amount Features (Average amount, ratio, std, large transaction)\n",
    "#         df['customer_avg_amt'] = df.groupby('customer_id')['amt'].transform('mean') # Average transaction amount per customer\n",
    "#         df['amt_ratio_avg'] = df['amt'] / (df['customer_avg_amt'] + 1e-9)  # Avoid division by zero, ratio of transaction amount to average\n",
    "#         df['customer_amt_std'] = df.groupby('customer_id')['amt'].transform('std').fillna(0) # Standard deviation of transaction amounts per customer\n",
    "#         df['large_transaction'] = (df['amt'] > df['customer_avg_amt'] + 3 * df['customer_amt_std']).astype(int) # Indicator if transaction is much larger than usual\n",
    "\n",
    "#         # Customer Behavior Features (Most used category, category count)\n",
    "#         df['customer_most_used_category'] = df.groupby('customer_id')['category'].transform(lambda x: x.mode()[0] if not x.mode().empty else np.nan) # Most frequent transaction category per customer\n",
    "#         df['customer_category_count'] = df.groupby(['customer_id', 'category'])['transaction_unique'].transform('count') # How many transactions per customer and category\n",
    "\n",
    "#         # Merchant Behavior Features (Merchant amount std and average)\n",
    "#         df['merchant_amt_std'] = df.groupby('merchant_id')['amt'].transform('std').fillna(0) # Standard deviation of transaction amounts per merchant\n",
    "#         df['merchant_avg_amt'] = df.groupby('merchant_id')['amt'].transform('mean') # Average transaction amount per merchant\n",
    "\n",
    "#         #Time since last category change, and previous category\n",
    "#         df['trans_date_trans_time'] = pd.to_datetime(df['trans_date_trans_time'], errors='coerce')\n",
    "#         df.sort_values(['customer_id', 'trans_date_trans_time'], inplace=True)\n",
    "#         df['prev_category'] = df.groupby('customer_id')['category'].shift(1).fillna(df['category']) # Previous category\n",
    "#         df['time_since_last_category_change'] = df.groupby('customer_id')['trans_date_trans_time'].diff().dt.total_seconds().fillna(0) # Time since customer changed categories\n",
    "\n",
    "#         #Transaction speed\n",
    "#         df['transaction_speed'] = df.groupby('customer_id')['trans_date_trans_time'].diff().dt.total_seconds().fillna(0) # Time between transactions\n",
    "\n",
    "#         #Merchant category fraud risk.\n",
    "#         merchant_cat_fraud = df.groupby('category')['is_fraud'].mean().to_dict()\n",
    "#         df['merchant_category_fraud_risk'] = df['category'].map(merchant_cat_fraud) #average fraud rate per category\n",
    "\n",
    "#         logger.info(\"Advanced features engineered.\") # Tells us new features were created\n",
    "#         return df\n",
    "\n",
    "#     def preprocess(self):\n",
    "#         df = self.load_data()\n",
    "#         if df is None:\n",
    "#             return\n",
    "\n",
    "#         df = self.drop_unwanted_columns(df)\n",
    "#         df = self.handle_missing_categorical(df)\n",
    "#         df = self.handle_numerical_features(df)\n",
    "#         df = self.normalize_numerical_features(df)\n",
    "#         df = self.create_customer_and_merchant_ids(df)\n",
    "#         df = self.extract_datetime_components(df)\n",
    "#         df = self.calculate_age(df)\n",
    "#         df = self.encode_data(df)\n",
    "\n",
    "#         df[\"transaction_unique\"] = range(len(df))\n",
    "\n",
    "#         df = self.feature_engineering(df)\n",
    "\n",
    "#         preprocessed_path = os.path.join(self.config.root_dir, \"transformed_dataset.csv\")\n",
    "#         df.to_csv(preprocessed_path, index=False)\n",
    "#         logger.info(f\"Preprocessed data saved at {preprocessed_path}\") # Tells us data was saved\n",
    "\n",
    "#         return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "from geopy.distance import geodesic\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class DataTransformation:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.label_encoders = {}\n",
    "        self.scaler = StandardScaler()\n",
    "        self.customer_mapping = {}\n",
    "        self.merchant_mapping = {}\n",
    "\n",
    "    def load_data(self):\n",
    "        try: return pd.read_csv(self.config.data_path)\n",
    "        except FileNotFoundError: logger.error(f\"File not found: {self.config.data_path}\"); return None\n",
    "\n",
    "    def handle_missing_values(self, df):\n",
    "        df[\"category\"] = df[\"category\"].fillna(\"unknown\")\n",
    "        df[\"state\"] = df[\"state\"].fillna(\"unknown\")\n",
    "        num_features = [\"amt\", \"city_pop\", \"lat\", \"long\", \"merch_lat\", \"merch_long\"]\n",
    "        existing_features = [col for col in num_features if col in df.columns]\n",
    "        df[existing_features] = df[existing_features].apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "        return df\n",
    "\n",
    "    def create_ids(self, df):\n",
    "        unique_customers = df['cc_num'].unique()\n",
    "        self.customer_mapping = {customer: idx for idx, customer in enumerate(unique_customers)}\n",
    "        df['customer_id'] = df['cc_num'].map(self.customer_mapping).astype(int)\n",
    "        unique_merchants = df['merchant'].unique()\n",
    "        self.merchant_mapping = {merchant: idx + len(unique_customers) for idx, merchant in enumerate(unique_merchants)}\n",
    "        df['merchant_id'] = df['merchant'].map(self.merchant_mapping).astype(int)\n",
    "        df.drop(columns=['cc_num', 'merchant'], inplace=True)\n",
    "        return df\n",
    "\n",
    "    def extract_datetime(self, df):\n",
    "        if 'trans_date_trans_time' in df.columns:\n",
    "            df['trans_date_trans_time'] = pd.to_datetime(df['trans_date_trans_time'])\n",
    "            df['trans_hour'] = df['trans_date_trans_time'].dt.hour\n",
    "        return df\n",
    "\n",
    "    def encode_categorical(self, df):\n",
    "        if \"gender\" in df.columns: df[\"gender\"] = df[\"gender\"].map({\"M\": 1, \"F\": 0})\n",
    "        for col in [\"category\", \"state\"]:\n",
    "            if col not in self.label_encoders: self.label_encoders[col] = LabelEncoder(); df[col] = self.label_encoders[col].fit_transform(df[col])\n",
    "            else: df[col] = self.label_encoders[col].transform(df[col])\n",
    "        return df\n",
    "\n",
    "    def engineer_features(self, df):\n",
    "        df[\"transaction_unique\"] = range(len(df))\n",
    "        df['customer_avg_amt'] = df.groupby('customer_id')['amt'].transform('mean')\n",
    "        df['merchant_category_fraud_risk'] = df.groupby('category')['is_fraud'].mean().to_dict()\n",
    "        df['merchant_category_fraud_risk'] = df['category'].map(df['merchant_category_fraud_risk'])\n",
    "        df['merchant_avg_amt'] = df.groupby('merchant_id')['amt'].transform('mean')\n",
    "        df['high_amt'] = (df['amt'] > df['customer_avg_amt'] + 3 * df.groupby('customer_id')['amt'].transform('std').fillna(0)).astype(int)\n",
    "        df['amt_ratio_merchant'] = df['amt'] / (df['merchant_avg_amt'] + 1e-9)\n",
    "        df['amt_diff_customer_avg'] = df['amt'] - df['customer_avg_amt']\n",
    "        df['hour_cos'] = np.cos(2 * np.pi * df['trans_hour'] / 24)\n",
    "        df['amt_per_city_pop'] = df['amt'] / (df['city_pop'] + 1e-9)\n",
    "        df['customer_min_amt'] = df.groupby('customer_id')['amt'].transform('min')\n",
    "        df['merchant_min_amt'] = df.groupby('merchant_id')['amt'].transform('min')\n",
    "        df['customer_amt_std'] = df.groupby('customer_id')['amt'].transform('std').fillna(0)\n",
    "        df['merchant_amt_std'] = df.groupby('merchant_id')['amt'].transform('std').fillna(0)\n",
    "        df['sqrt_amt'] = np.sqrt(df['amt'])\n",
    "        return df\n",
    "\n",
    "    def select_final_features(self, df):\n",
    "        columns_to_keep = ['high_amt', 'amt_ratio_merchant', 'sqrt_amt', 'amt', 'customer_avg_amt','amt_diff_customer_avg', 'hour_cos', 'amt_per_city_pop', 'customer_min_amt','merchant_category_fraud_risk', 'merchant_avg_amt', 'merchant_min_amt','customer_amt_std', 'merchant_amt_std', 'customer_id','merchant_id','transaction_unique', 'is_fraud']\n",
    "        return df[columns_to_keep]\n",
    "\n",
    "    def normalize_features(self, df):\n",
    "        num_features_to_scale = [col for col in df.select_dtypes(include=np.number).columns.tolist() if col not in ['is_fraud', 'customer_id', 'merchant_id','transaction_unique']]\n",
    "        df[num_features_to_scale] = self.scaler.fit_transform(df[num_features_to_scale]).round(5)\n",
    "        return df\n",
    "\n",
    "    def preprocess(self):\n",
    "        df = self.load_data()\n",
    "        if df is None: return\n",
    "        df = self.handle_missing_values(df)\n",
    "        df = self.create_ids(df)\n",
    "        df = self.extract_datetime(df)\n",
    "        df = self.encode_categorical(df)\n",
    "        df = self.engineer_features(df)\n",
    "        df = self.select_final_features(df)\n",
    "        df = self.normalize_features(df)\n",
    "\n",
    "        preprocessed_path = os.path.join(self.config.root_dir, \"transformed_dataset.csv\")\n",
    "        df.to_csv(preprocessed_path, index=False)\n",
    "        logger.info(f\"Preprocessed data saved at {preprocessed_path}\")\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-22 21:54:04,648: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-03-22 21:54:04,655: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-03-22 21:54:04,656: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2025-03-22 21:54:04,657: INFO: common: created directory at: artifacts]\n",
      "[2025-03-22 21:54:04,658: INFO: common: created directory at: artifacts/data_transformation]\n",
      "[2025-03-22 21:54:20,887: INFO: 2722166197: Preprocessed data saved at artifacts/data_transformation\\transformed_dataset.csv]\n"
     ]
    }
   ],
   "source": [
    "# Pipeline\n",
    "\n",
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "    data_transformation = DataTransformation(config=data_transformation_config)\n",
    "    transformed_data = data_transformation.preprocess()  # Corrected method call\n",
    "except Exception as e:\n",
    "    raise e  # This will re-raise the exception if any error occurs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
