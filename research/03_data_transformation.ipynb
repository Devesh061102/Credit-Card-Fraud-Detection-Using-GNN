{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d:\\\\Final-Year-Project\\\\Credit-Card-Fraud-Detection-Using-GNN'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os  # Importing the operating system module to interact with the system\n",
    "os.chdir(\"../\")  # Changing the working directory to one level up\n",
    "%pwd  # Displaying the current working directory \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle  # Used for saving and loading Python objects\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "# Importing constants and utility functions\n",
    "from Credit_Card_Fraud_Detection.constants import *\n",
    "from Credit_Card_Fraud_Detection.utils.common import read_yaml, create_directories\n",
    "from Credit_Card_Fraud_Detection import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# ENTITY: DataTransformationConfig\n",
    "# ====================================================\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class DataTransformationConfig:\n",
    "    \"\"\"\n",
    "    This class stores configuration details for data transformation.\n",
    "    - root_dir: Main directory where transformed data is stored.\n",
    "    - data_path: Path to the raw data.\n",
    "    - customer_mapping_path: Path to store customer ID mappings.\n",
    "    - merchant_mapping_path: Path to store merchant ID mappings.\n",
    "    - label_encoders_path: Path to store label encoders.\n",
    "    - scaler_path: Path to store the scaler for normalization.\n",
    "    \"\"\"\n",
    "    root_dir: Path\n",
    "    data_path: Path\n",
    "    customer_mapping_path: Path\n",
    "    merchant_mapping_path: Path\n",
    "    label_encoders_path: Path\n",
    "    scaler_path: Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# CONFIGURATION MANAGER\n",
    "# ====================================================\n",
    "\n",
    "class ConfigurationManager:\n",
    "    \"\"\"\n",
    "    This class manages configuration settings by reading YAML files.\n",
    "    It loads config, parameters, and schema details.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        config_filepath=CONFIG_FILE_PATH,\n",
    "        params_filepath=PARAMS_FILE_PATH,\n",
    "        schema_filepath=SCHEMA_FILE_PATH):\n",
    "        \n",
    "        # Read YAML configuration files\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "        self.schema = read_yaml(schema_filepath)\n",
    "\n",
    "        # Create required directories\n",
    "        create_directories([self.config.data_transformation.root_dir])\n",
    "\n",
    "    def get_data_transformation_config(self) -> DataTransformationConfig:\n",
    "        \"\"\"Retrieves data transformation settings and ensures directories exist.\"\"\"\n",
    "        config = self.config.data_transformation\n",
    "        create_directories([config.root_dir])\n",
    "\n",
    "        return DataTransformationConfig(\n",
    "            root_dir=Path(config.root_dir),\n",
    "            data_path=Path(config.data_path),\n",
    "            customer_mapping_path=Path(config.customer_mapping_file),\n",
    "            merchant_mapping_path=Path(config.merchant_mapping_file),\n",
    "            label_encoders_path=Path(config.label_encoders_file),\n",
    "            scaler_path=Path(config.scaler_file)\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====================================================\n",
    "# COMPONENT: Data Transformation\n",
    "# ====================================================\n",
    "\n",
    "class DataTransformation:\n",
    "    \"\"\"\n",
    "    This class handles data preprocessing, including:\n",
    "    - Loading data,\n",
    "    - Handling missing values,\n",
    "    - Assigning unique IDs,\n",
    "    - Extracting datetime features,\n",
    "    - Encoding categorical variables,\n",
    "    - Engineering new features,\n",
    "    - Selecting final features,\n",
    "    - Normalizing numerical features.\n",
    "    \"\"\"\n",
    "    def __init__(self, config, customer_mapping=None, merchant_mapping=None, label_encoders=None, scaler=None):\n",
    "        \"\"\"\n",
    "        Initializes the DataTransformation class with the provided configuration and optional mappings/encoders.\n",
    "\n",
    "        Args:\n",
    "            config: Configuration object containing data transformation details.\n",
    "            customer_mapping (dict, optional): Mapping of customer IDs. Defaults to None.\n",
    "            merchant_mapping (dict, optional): Mapping of merchant IDs. Defaults to None.\n",
    "            label_encoders (dict, optional): Dictionary of LabelEncoders for categorical features. Defaults to None.\n",
    "            scaler (StandardScaler, optional): StandardScaler object for normalization. Defaults to None.\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.label_encoders = label_encoders or {}  # Use provided or create empty dict\n",
    "        self.scaler = scaler or StandardScaler()  # Use provided or create new StandardScaler\n",
    "        self.customer_mapping = customer_mapping or {}  # Use provided or create empty dict\n",
    "        self.merchant_mapping = merchant_mapping or {}  # Use provided or create empty dict\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Loads the dataset from the specified path.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: Loaded DataFrame if successful, None otherwise.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            return pd.read_csv(self.config.data_path)  # Load data from CSV file\n",
    "        except FileNotFoundError:\n",
    "            logger.error(f\"File not found: {self.config.data_path}\")  # Log error if file not found\n",
    "            return None  # Return None if loading fails\n",
    "\n",
    "    def handle_missing_values(self, df):\n",
    "        \"\"\"\n",
    "        Fills missing values for categorical and numerical columns.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): DataFrame to handle missing values.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame with filled missing values.\n",
    "        \"\"\"\n",
    "        df.fillna({\"category\": \"unknown\", \"state\": \"unknown\"}, inplace=True)  # Fill categorical missing values with 'unknown'\n",
    "        num_features = [\"amt\", \"city_pop\", \"lat\", \"long\", \"merch_lat\", \"merch_long\"]  # Numerical features to handle\n",
    "        df[num_features] = df[num_features].apply(pd.to_numeric, errors='coerce').fillna(0)  # Convert to numeric, fill NaN with 0\n",
    "        return df\n",
    "\n",
    "    def create_ids(self, df):\n",
    "        \"\"\"\n",
    "        Assigns unique numerical IDs to customers and merchants.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): DataFrame to assign IDs.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame with assigned customer and merchant IDs.\n",
    "        \"\"\"\n",
    "        self.customer_mapping = {customer: idx for idx, customer in enumerate(df['cc_num'].unique())}  # Create mapping for customers\n",
    "        df['customer_id'] = df['cc_num'].map(self.customer_mapping)  # Map customer IDs\n",
    "\n",
    "        self.merchant_mapping = {merchant: idx + len(self.customer_mapping) for idx, merchant in enumerate(df['merchant'].unique())}  # Create mapping for merchants\n",
    "        df['merchant_id'] = df['merchant'].map(self.merchant_mapping)  # Map merchant IDs\n",
    "        \n",
    "        df.drop(columns=['cc_num', 'merchant'], inplace=True)  # Drop original customer and merchant columns\n",
    "        return df\n",
    "\n",
    "    def extract_datetime(self, df):\n",
    "        \"\"\"\n",
    "        Extracts hour information from the transaction timestamp.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): DataFrame with transaction timestamps.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame with extracted hour information.\n",
    "        \"\"\"\n",
    "        if 'trans_date_trans_time' in df.columns:  # Check if timestamp column exists\n",
    "            df['trans_date_trans_time'] = pd.to_datetime(df['trans_date_trans_time'])  # Convert to datetime\n",
    "            df['trans_hour'] = df['trans_date_trans_time'].dt.hour  # Extract hour\n",
    "        return df\n",
    "\n",
    "    def encode_categorical(self, df):\n",
    "        \"\"\"\n",
    "        Encodes categorical variables using Label Encoding.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): DataFrame with categorical columns.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame with encoded categorical columns.\n",
    "        \"\"\"\n",
    "        df[\"gender\"] = df[\"gender\"].map({\"M\": 1, \"F\": 0}) if \"gender\" in df.columns else df[\"gender\"] # Map gender to numerical if exists.\n",
    "        for col in [\"category\", \"state\"]:  # Loop through categorical columns\n",
    "            if col not in self.label_encoders:  # If encoder not already created\n",
    "                self.label_encoders[col] = LabelEncoder()  # Create new LabelEncoder\n",
    "                df[col] = self.label_encoders[col].fit_transform(df[col])  # Fit and transform\n",
    "            else:\n",
    "                df[col] = self.label_encoders[col].transform(df[col])  # Transform using existing encoder\n",
    "        return df\n",
    "\n",
    "    def engineer_features(self, df):\n",
    "        \"\"\"\n",
    "        Creates new features for better fraud detection analysis.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): DataFrame to engineer features on.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame with new engineered features.\n",
    "        \"\"\"\n",
    "        df[\"transaction_unique\"] = range(len(df))  # Unique transaction ID\n",
    "        df[\"customer_avg_amt\"] = df.groupby(\"customer_id\")[\"amt\"].transform(\"mean\")  # Average amount per customer\n",
    "        df[\"merchant_avg_amt\"] = df.groupby(\"merchant_id\")[\"amt\"].transform(\"mean\")  # Average amount per merchant\n",
    "        df[\"high_amt\"] = (df[\"amt\"] > df[\"customer_avg_amt\"] + 3 * df.groupby(\"customer_id\")[\"amt\"].transform(\"std\").fillna(0)).astype(int)  # High amount flag\n",
    "        df[\"amt_ratio_merchant\"] = df[\"amt\"] / (df[\"merchant_avg_amt\"] + 1e-9)  # Amount ratio to merchant average\n",
    "        df[\"amt_diff_customer_avg\"] = df[\"amt\"] - df[\"customer_avg_amt\"]  # Amount difference from customer average\n",
    "        df[\"hour_cos\"] = np.cos(2 * np.pi * df[\"trans_hour\"] / 24)  # Cosine transformation of hour\n",
    "        df[\"amt_per_city_pop\"] = df[\"amt\"] / (df[\"city_pop\"] + 1e-9)  # Amount per city population\n",
    "        df[\"customer_min_amt\"] = df.groupby(\"customer_id\")[\"amt\"].transform(\"min\")  # Minimum amount per customer\n",
    "        df[\"merchant_min_amt\"] = df.groupby(\"merchant_id\")[\"amt\"].transform(\"min\")  # Minimum amount per merchant\n",
    "        df[\"customer_amt_std\"] = df.groupby(\"customer_id\")[\"amt\"].transform(\"std\").fillna(0)  # Standard deviation of amount per customer\n",
    "        df[\"merchant_amt_std\"] = df.groupby(\"merchant_id\")[\"amt\"].transform(\"std\").fillna(0)  # Standard deviation of amount per merchant\n",
    "        df[\"sqrt_amt\"] = np.sqrt(df[\"amt\"]) # Square root of amount.\n",
    "        return df\n",
    "\n",
    "    def select_final_features(self, df):\n",
    "        \"\"\"\n",
    "        Selects only relevant features for modeling.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): DataFrame to select features from.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame with selected features.\n",
    "        \"\"\"\n",
    "        columns_to_keep = ['high_amt', 'amt_ratio_merchant', 'sqrt_amt', 'amt', 'customer_avg_amt',\n",
    "                            'amt_diff_customer_avg', 'hour_cos', 'amt_per_city_pop', 'customer_id', 'merchant_id',\n",
    "                            'merchant_avg_amt','merchant_min_amt','customer_min_amt','customer_amt_std','merchant_amt_std','transaction_unique']\n",
    "        if 'is_fraud' in df.columns:  # If target column exists\n",
    "            columns_to_keep.append('is_fraud')  # Add target column\n",
    "        return df[columns_to_keep]\n",
    "\n",
    "    def normalize_features(self, df):\n",
    "        \"\"\"\n",
    "        Normalizes numerical features using StandardScaler.\n",
    "\n",
    "        Args:\n",
    "            df (pd.DataFrame): DataFrame to normalize.\n",
    "\n",
    "        Returns:\n",
    "            pd.DataFrame: DataFrame with normalized features.\n",
    "        \"\"\"\n",
    "        num_features = [col for col in df.select_dtypes(include=np.number) if col not in ['is_fraud', 'customer_id', 'merchant_id', 'transaction_unique']]  # Select numerical columns\n",
    "        df[num_features] = self.scaler.fit_transform(df[num_features]).round(5)  # Normalize and round to 5 decimal places\n",
    "        return df\n",
    "\n",
    "    def preprocess(self):\n",
    "        \"\"\"\n",
    "        Main function to execute the full preprocessing pipeline.\n",
    "        Loads, cleans, transforms, and saves the processed dataset.\n",
    "\n",
    "        Returns:\n",
    "            tuple: customer_mapping, merchant_mapping, label_encoders, scaler if successful, else tuple of None.\n",
    "        \"\"\"\n",
    "        df = self.load_data()  # Load data\n",
    "        if df is None:  # If loading failed\n",
    "            return None, None, None, None  # Return None for all mappings and encoders\n",
    "        \n",
    "        df = self.handle_missing_values(df)  # Handle missing values\n",
    "        df = self.create_ids(df)  # Create customer and merchant IDs\n",
    "        df = self.extract_datetime(df)  # Extract datetime features\n",
    "        df = self.encode_categorical(df)  # Encode categorical features\n",
    "        df = self.engineer_features(df)  # Engineer new features\n",
    "        df = self.select_final_features(df)  # Select final features\n",
    "        df = self.normalize_features(df)  # Normalize numerical features\n",
    "        \n",
    "        preprocessed_path = os.path.join(self.config.root_dir, \"transformed_dataset.csv\")  # Define output path\n",
    "        df.to_csv(preprocessed_path, index=False)  # Save preprocessed data to CSV\n",
    "        logger.info(f\"Preprocessed data saved at {preprocessed_path}\")  # Log successful saving\n",
    "        \n",
    "        return self.customer_mapping, self.merchant_mapping, self.label_encoders, self.scaler  # Return mappings and encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-26 10:58:27,161: INFO: common: yaml file: config\\config.yaml loaded successfully]\n",
      "[2025-03-26 10:58:27,164: INFO: common: yaml file: params.yaml loaded successfully]\n",
      "[2025-03-26 10:58:27,165: INFO: common: yaml file: schema.yaml loaded successfully]\n",
      "[2025-03-26 10:58:27,165: INFO: common: created directory at: artifacts/data_transformation]\n",
      "[2025-03-26 10:58:27,168: INFO: common: created directory at: artifacts/data_transformation]\n",
      "[2025-03-26 10:58:42,846: INFO: 2449968333: Preprocessed data saved at artifacts\\data_transformation\\transformed_dataset.csv]\n"
     ]
    }
   ],
   "source": [
    "# ===========================================\n",
    "# TRAINING DATA PIPELINE\n",
    "# ===========================================\n",
    "try:\n",
    "    # Step 1: Load configuration settings\n",
    "    config = ConfigurationManager()\n",
    "    data_transformation_config = config.get_data_transformation_config()\n",
    "\n",
    "    # Step 2: Initialize the DataTransformation class with the configuration\n",
    "    data_transformation = DataTransformation(config=data_transformation_config)\n",
    "\n",
    "    # Step 3: Perform data preprocessing\n",
    "    customer_mapping, merchant_mapping, label_encoders, scaler = data_transformation.preprocess()\n",
    "\n",
    "    # Step 4: Save transformation artifacts (customer/merchant mappings, encoders, and scaler)\n",
    "    with open(data_transformation_config.customer_mapping_path, 'wb') as f:\n",
    "        pickle.dump(customer_mapping, f)\n",
    "\n",
    "    with open(data_transformation_config.merchant_mapping_path, 'wb') as f:\n",
    "        pickle.dump(merchant_mapping, f)\n",
    "\n",
    "    with open(data_transformation_config.label_encoders_path, 'wb') as f:\n",
    "        pickle.dump(label_encoders, f)\n",
    "\n",
    "    with open(data_transformation_config.scaler_path, 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "\n",
    "except Exception as e:\n",
    "    # Handle and raise any unexpected exceptions\n",
    "    raise RuntimeError(f\"Error in training pipeline: {str(e)}\") from e\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlproj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
